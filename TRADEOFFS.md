# Design Trade-offs and Decisions

This document explains the key architectural decisions, trade-offs considered, and rationale behind the PDF Parser implementation.

## 1. Chunking Strategy

### Decision: Sentence-Boundary Aware Chunking

**Alternatives Considered**:
1. **Fixed-size chunking** (e.g., every 1000 characters)
2. **Paragraph-based chunking**
3. **Sliding window with fixed overlap**
4. **Semantic chunking** (ML-based topic detection)

**Why Sentence-Boundary**:
- âœ… Preserves semantic meaning (doesn't cut mid-sentence)
- âœ… Flexible enough to handle various document structures
- âœ… Simple to implement without ML dependencies
- âœ… Good balance between precision and context
- âŒ Can create variable-sized chunks (but within acceptable range)

**Trade-off**: Slightly more complex than fixed-size, but significantly better for downstream tasks (search, embeddings, Q&A).

---

## 2. Token Count Target: 600-1200 tokens

### Decision: 600-1200 token range per chunk

**Alternatives Considered**:
- 256-512 tokens (smaller, more precise)
- 1500-2000 tokens (larger, more context)
- No limit (paragraph-based)

**Why 600-1200**:
- âœ… Matches common embedding model limits (1536 for OpenAI)
- âœ… Large enough to preserve context
- âœ… Small enough for precise retrieval
- âœ… Leaves room for prompt engineering (system prompts + chunk)
- âœ… Balances storage cost with retrieval quality

**Trade-off**: Larger chunks = more context but less precise search. Our range optimizes for RAG (Retrieval-Augmented Generation) use cases.

---

## 3. Overlap: 10-15%

### Decision: 10-15% overlap between consecutive chunks

**Alternatives Considered**:
- 0% overlap (no duplication)
- 25-50% overlap (high redundancy)
- Fixed token overlap (e.g., always 100 tokens)

**Why 10-15%**:
- âœ… Prevents information loss at chunk boundaries
- âœ… Minimal storage overhead (~12% increase)
- âœ… Improves context continuity for embeddings
- âœ… Catches references that span boundaries
- âŒ Slight duplication in storage and search results

**Trade-off**: Small storage cost for significant improvement in retrieval quality. Critical for cases where key information spans chunk boundaries.

---

## 4. Idempotency: Content Hash + Unique Constraint

### Decision: SHA-256 hash of content with DB constraint

**Alternatives Considered**:
1. **Delete all + rewrite** (simpler, chosen initially)
2. **Upsert by hash** (more complex queries)
3. **Sequence number versioning** (keep history)
4. **No idempotency** (allow duplicates)

**Why Content Hash**:
- âœ… Simple and deterministic
- âœ… DB-enforced uniqueness (can't accidentally duplicate)
- âœ… Works even if chunk order changes
- âœ… Fast lookups (indexed hash)
- âŒ Extra 64 bytes per chunk for hash storage

**Implementation**: "Clear + Rewrite"
- Delete existing chunks for document_id before inserting new ones
- Simpler than upsert logic
- Acceptable for our use case (re-parsing is infrequent)

**Trade-off**: Storing hash uses ~64 bytes per chunk, but prevents costly duplicates and enables fast idempotency checks.

---

## 5. Token Counting: Character-based Approximation

### Decision: Simple approximation (1 token â‰ˆ 4 characters)

**Alternatives Considered**:
1. **tiktoken library** (accurate but requires Node.js native modules)
2. **Word-based approximation** (1 token â‰ˆ 0.75 words)
3. **GPT-3 tokenizer API** (accurate but slow, costs money)
4. **No token counting** (just character count)

**Why Simple Approximation**:
- âœ… Works in Deno Edge Functions (no native modules)
- âœ… Fast (no external dependencies)
- âœ… Sufficient accuracy for chunking (~10% error)
- âœ… No API calls or external dependencies
- âŒ Not exact for embeddings/LLM cost estimation

**When to Upgrade**:
- If using embeddings: switch to tiktoken with cl100k_base encoding
- If billing by tokens: use model-specific tokenizer
- For now: approximation is good enough for chunking

**Trade-off**: Accuracy vs. simplicity. For chunking boundaries, approximation is sufficient. For billing/embeddings, upgrade to tiktoken.

---

## 6. Search: pg_trgm Full-Text Search

### Decision: PostgreSQL trigram similarity search

**Alternatives Considered**:
1. **Basic LIKE queries** (slow, no fuzzy matching)
2. **PostgreSQL FTS (tsvector)** (good, but requires language config)
3. **Elasticsearch** (powerful but adds complexity)
4. **Vector embeddings + similarity search** (semantic but expensive)

**Why pg_trgm**:
- âœ… Built into PostgreSQL (no extra infrastructure)
- âœ… Fuzzy matching (handles typos)
- âœ… Fast with GIN indexes
- âœ… Simple to implement
- âŒ Not semantic (won't match synonyms)
- âŒ Less powerful than dedicated search engines

**Future Upgrade Path**:
- Add pgvector for semantic search
- Hybrid search: keyword (pg_trgm) + semantic (vectors)
- Keep pg_trgm for exact phrase matching

**Trade-off**: Good enough for MVP, easy to upgrade later. Avoids premature optimization.

---

## 7. Storage: Supabase Storage vs. Database BLOBs

### Decision: Store PDFs in Supabase Storage, chunks in PostgreSQL

**Alternatives Considered**:
1. **Store PDFs as BLOBs in PostgreSQL**
2. **Store chunks in separate JSON files in Storage**
3. **External S3 bucket**

**Why Supabase Storage**:
- âœ… Cheaper for large files ($0.021/GB vs. DB cost)
- âœ… Better for streaming/downloading
- âœ… Signed URLs for secure access
- âœ… Separate scaling (storage vs. DB)
- âœ… Built-in CDN

**Why PostgreSQL for Chunks**:
- âœ… Excellent for searchable text
- âœ… ACID guarantees
- âœ… RLS for security
- âœ… Powerful querying (JOIN, aggregate, full-text)
- âœ… Indexes for fast search

**Trade-off**: Two systems to manage, but optimal cost and performance for each data type.

---

## 8. Processing: Edge Function vs. Background Jobs

### Decision: Supabase Edge Function (synchronous-ish)

**Alternatives Considered**:
1. **Serverless function (AWS Lambda, Vercel)**
2. **Background job queue (BullMQ, Supabase Queue)**
3. **Client-side processing (WebAssembly PDF.js)**
4. **Dedicated worker server**

**Why Edge Function**:
- âœ… Integrated with Supabase (auth, storage, DB)
- âœ… No separate infrastructure
- âœ… Auto-scaling
- âœ… Cost-effective (pay per invocation)
- âŒ 60-second timeout (limits very large PDFs)
- âŒ Cold start latency

**When to Upgrade**:
- PDFs > 500 pages: background queue
- High concurrency: dedicated workers
- Complex ML (OCR, embeddings): GPU workers

**Trade-off**: Simplicity and cost-effectiveness vs. processing limits. Edge Functions perfect for 80% of use cases.

---

## 9. User Authentication: Demo User vs. Real Auth

### Current: Fixed demo user_id

**Why Not Real Auth (for now)**:
- âœ… Simplifies demo/testing
- âœ… Focuses on core chunking logic
- âœ… Easy to add later (just replace user_id)
- âŒ NOT production-ready
- âŒ All users share data (demo mode)

**Production Upgrade**:
```typescript
// Get user from Supabase Auth session
const { data: { user } } = await supabase.auth.getUser();
const userId = user.id;
```

**Trade-off**: Ship faster with demo, add auth later. RLS policies already in place, just need to connect real users.

---

## 10. Polling vs. WebSockets for Status Updates

### Decision: HTTP polling every 2 seconds

**Alternatives Considered**:
1. **WebSockets (real-time)**
2. **Server-Sent Events (SSE)**
3. **Long polling**
4. **Webhook callbacks**

**Why Polling**:
- âœ… Simple to implement
- âœ… Works everywhere (no WebSocket compatibility issues)
- âœ… Stateless (no connection management)
- âœ… Sufficient for our use case (processing takes ~30s)
- âŒ Not true real-time
- âŒ More API calls (but minimal with 2s interval)

**When to Upgrade**:
- High user count: WebSockets or SSE
- Real-time dashboard: WebSockets
- For now: polling is fine (processing is relatively slow anyway)

**Trade-off**: Simplicity vs. real-time. Polling adds negligible load for our scale.

---

## 11. OCR: Not Implemented (Documented Approach)

### Decision: Detect but don't process image-only pages

**Why Not Implement OCR**:
- â±ï¸ Time-boxed project (4-6 hours)
- ğŸ’° OCR APIs cost money or are slow
- ğŸ¯ Core focus was chunking, not OCR
- ğŸ“ Documented approach for future implementation

**Recommended Approach** (when needed):
1. **Detect**: `if (pageText.length < 10) { /* image-only */ }`
2. **Tesseract.js** for MVP (free, open-source)
3. **Cloud OCR** for production (Google Vision, AWS Textract)

**Trade-off**: Shipped core functionality faster, OCR is an additive feature.

---

## 12. Testing: Unit Tests for Chunker Only

### Decision: Comprehensive unit tests, documented integration tests

**What's Tested**:
- âœ… Chunking logic (boundaries, overlap, tokens)
- âœ… Edge cases (empty, special chars, single sentence)
- âœ… Token counting accuracy
- âœ… Content hashing

**What's Not Tested (yet)**:
- âŒ API routes (integration tests)
- âŒ Edge Function (E2E tests)
- âŒ UI components (React testing)

**Why This Approach**:
- âœ… Chunker is the most critical logic (deserves thorough testing)
- âœ… Pure functions are easy to test
- âœ… Integration tests documented in README (manual testing)
- â±ï¸ Time-boxed project

**Future Testing**:
- Add Playwright for E2E
- Add MSW for API mocking
- Add Supabase local dev for integration tests

**Trade-off**: Focus testing effort on highest-value, most complex logic.

---

## Summary: Key Trade-offs

| Decision | Simplicity | Performance | Cost | Maintainability |
|----------|-----------|-------------|------|-----------------|
| Sentence-boundary chunking | â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ |
| 600-1200 token chunks | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ |
| Content hash idempotency | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ |
| Character-based tokens | â­â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | â­â­â­ |
| pg_trgm search | â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | â­â­â­â­ |
| Edge Functions | â­â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | â­â­â­â­ |
| HTTP polling | â­â­â­â­â­ | â­â­â­ | â­â­â­â­ | â­â­â­â­â­ |

**Overall Philosophy**: Ship a working, well-architected MVP fast. Each decision has a clear upgrade path for when scale demands it.

---

## What I'd Do Differently at Scale

### At 1,000+ users:
- Add WebSocket for real-time updates
- Implement caching (Redis) for frequent searches
- Add rate limiting and quotas
- Use tiktoken for accurate token counts

### At 10,000+ users:
- Background job queue for processing
- Dedicated workers with GPUs (for OCR/embeddings)
- Elasticsearch or Algolia for search
- CDN for PDF downloads
- Comprehensive monitoring (Datadog, Sentry)

### At 100,000+ users:
- Microservices architecture
- Separate DB for chunks (time-series DB?)
- Vector DB for semantic search (Pinecone, Weaviate)
- Multi-region deployment
- Auto-scaling worker pools

---

**The beauty of this architecture**: It works great now, and there's a clear path to scale. No premature optimization, but also no dead ends.

